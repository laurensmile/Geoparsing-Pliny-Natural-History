{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35e2ac4",
   "metadata": {},
   "source": [
    "# Evaluate Flair NER on Book 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b759887",
   "metadata": {},
   "source": [
    "To evalute the quality of the Flair NER annotation in Book 4, we calculated the Precision, Recall and F1 Score against a manually curated Gold Standard for Book 4. In general, Flair NER contains a high-quality annotation (F1 0.946) with a high Precision (0.937) and Recall (0.955).\n",
    "\n",
    "In comparison to the ToposText annotation (Precision: 0.985, Recall: 0.747), the Flair NER is less precise, but more sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c466c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf09a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open the Gold Standard for Book 4 (2,491 entries)\n",
    "\n",
    "GoldStandard_Book4 = pd.read_excel(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/1.3.GoldStandard_Book4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b249b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2491"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GoldStandard_Book4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae4b1ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## open the output of Flair NER (2,539 entries)\n",
    "\n",
    "NER_Flair_Book4 = pd.read_csv(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/1.4.NER_Flair_Book_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6547c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2539"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NER_Flair_Book4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59b26a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a set of tuples for the Gold Standard\n",
    "GoldStandard_tuples = set(zip(GoldStandard_Book4['Reference'], GoldStandard_Book4['First position']))\n",
    "\n",
    "## create a set of tuples for the Flair NER\n",
    "NER_Flair_tuples = set(zip(NER_Flair_Book4['Reference'], NER_Flair_Book4['First position']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e340d24",
   "metadata": {},
   "source": [
    "The Flair NER annotation contains 2,381 true positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8550ca2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2381"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate true positives\n",
    "\n",
    "True_Positives = len(NER_Flair_tuples.intersection(GoldStandard_tuples))\n",
    "True_Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395cc7d",
   "metadata": {},
   "source": [
    "The Flair NER annotation contains 158 false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4baff854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate false positives\n",
    "\n",
    "False_Positives = len(NER_Flair_tuples) - True_Positives\n",
    "False_Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b434da",
   "metadata": {},
   "source": [
    "The Flair NER annotation had a precision of 0.937."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a269f91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9377707758960221"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate the precision\n",
    "\n",
    "Precision = True_Positives / (True_Positives + False_Positives)\n",
    "Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bef267",
   "metadata": {},
   "source": [
    "The Flair NER annotation contains 110 false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbebde0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate false negatives\n",
    "\n",
    "False_Negatives = len(GoldStandard_tuples - NER_Flair_tuples)\n",
    "False_Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decc309",
   "metadata": {},
   "source": [
    "The Flair NER annotation had a recall of 0.955."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91d86e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.955841027699719"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate recall\n",
    "\n",
    "Recall = True_Positives / (True_Positives + False_Negatives)\n",
    "Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf34a2",
   "metadata": {},
   "source": [
    "The Flair NER had a F1 score of 0.946."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4adcec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9467196819085488"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate F1\n",
    "\n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
