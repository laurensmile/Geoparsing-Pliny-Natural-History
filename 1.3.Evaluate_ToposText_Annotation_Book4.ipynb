{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5348c9b",
   "metadata": {},
   "source": [
    "# Evaluation of ToposText Annotation in Book 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da7682",
   "metadata": {},
   "source": [
    "To evalute the quality of the ToposText annotation, Precision, Recall and F1 Score were calculated on the basis of a manually curated Gold Standard for Book 4. In general, ToposText contains a high-quality annotation (F1 0.849) with a very high Precision (0.985). The Recall, instead, is lower (0.747) indicating that some entities were not annotated. In total, 630 false negatives were found.\n",
    "\n",
    "For an introduction to these metrics see: https://towardsdatascience.com/a-look-at-precision-recall-and-f1-score-36b5fd0dd3ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6928be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d12cf",
   "metadata": {},
   "source": [
    "# 1.3.1 Creation of the Gold Standard of Book 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adefce1",
   "metadata": {},
   "source": [
    "The Gold Standard - created to evalute the quality annotation in ToposText and, in the following step, of a NER system - was manually curated. Consistent and accurate guidelines were provided during the annotation process to avoid inconsistencies. This was particularly relevant, since no clear guidelines are provided for the ToposText annotation. \n",
    "\n",
    "Book 4 is representative of the entity types to annotate, since it is a book on geography.\n",
    "\n",
    "The entities annotated in the Gold Standard are: places (LOC) such as 'Rome', 'Athens'; persons (PEO) such as 'Caesar', 'Jupiter', and groups of people (GRO) such as 'Atraces', 'Corinthians'. Ehtnics were annotated only when they are referred to groups of individuals and not to a single person (i.e., in the sentence 'Perikles the Athenian', 'Athenian' was not annotated). It was observed, indeed, that some geographic regions are not named by a toponym, but by the name of the group of people living there. Adjectives were annotated when they are referred to places, such as 'Ionic (sea)'. Finally, only proper names were annotated. To make an example, in the sentence 'the Mount Pindus' only the word 'Pindus' was annotated. These entity boundaries (mostly consisting of single words rather than multi-word entities) are preferable to switch from the English to the Latin text (see next notebooks) by the Python library FuzzyWuzzy.\n",
    "\n",
    "The Gold Standard contains a consistent annotation format that consists of the reference (book, chapter, paragraph), the named entity, the label, the start and end position of the word in the paragraph.\n",
    "\n",
    "The Gold Standard contains 2,491 entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f74df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open the Gold Standard of Book 4 (2,491 entries)\n",
    "GoldStandard_Book4 = pd.read_excel(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/1.3.GoldStandard_Book4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e5668c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2491"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GoldStandard_Book4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7c8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open the file containing the ToposText annotations in Book 4 (1,888 rows)\n",
    "ToposText_Book4 = pd.read_csv(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/1.1.ToposText_Annotations_Book_4.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f6b0c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1888"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ToposText_Book4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0cb5d5",
   "metadata": {},
   "source": [
    "# 1.3.2 Calculate Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f9d1a",
   "metadata": {},
   "source": [
    "To calculate Precision, Recall and F1 Score, a set of tuples was generated for both the Gold Standard and the ToposText annotations. The tuples consist of two elements: the reference (book, chapter) and the start position of each annotation. An example of the tuples generated is: 'urn:cts:latinLit:phi0978.phi001:4.23.3', 330. The text of the tagged entity, instead, was not included in the tuple (i.e., 'urn:cts:latinLit:phi0978.phi001:4.1.1', 75, Acroceraunia). In some case, indeed, it was observed that the annotation is correct (same start position) but the text annotated is different from the ground truth (i.e., Corinthian / Cortinthian Gulf). Considering only the reference and the start position permits us to reduce the miscounting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "286aba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a set of tuples for the Gold Standard\n",
    "GoldStandard_tuples = set(zip(GoldStandard_Book4['Reference'], GoldStandard_Book4['First position']))\n",
    "\n",
    "## create a set of tuples for the ToposText annotations\n",
    "ToposText_tuples = set(zip(ToposText_Book4['Reference'], ToposText_Book4['First position']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfcdd75",
   "metadata": {},
   "source": [
    "The intersection operation determines the common elements between the two sets and identify the named entities that are correctly identified by comparing the predicted entities with the ground truth entities.\n",
    "\n",
    "The ToposText annotation contains 1,861 true positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2851ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1861"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate true positives\n",
    "\n",
    "True_Positives = len(ToposText_tuples.intersection(GoldStandard_tuples))\n",
    "True_Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd1ea11",
   "metadata": {},
   "source": [
    "The ToposText annotation contains 27 false positive. An example of false positive is 'canal' (urn:cts:latinLit:phi0978.phi001:4.5.1, 1538)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea5dd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate false positives\n",
    "\n",
    "False_Positives = len(ToposText_tuples) - True_Positives\n",
    "False_Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae21b18",
   "metadata": {},
   "source": [
    "The ToposText annotation had a precision of 0.98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce2e0965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9856991525423728"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate precision\n",
    "\n",
    "Precision = True_Positives / (True_Positives + False_Positives)\n",
    "Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866502e",
   "metadata": {},
   "source": [
    "The ToposText annotation contains 630 false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f9a475c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate false negatives\n",
    "\n",
    "False_Negatives = len(GoldStandard_tuples - ToposText_tuples)\n",
    "False_Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ba693",
   "metadata": {},
   "source": [
    "The ToposText annotation had a recall of 0.74."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4a016c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7470895222802088"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate recall\n",
    "\n",
    "Recall = True_Positives / (True_Positives + False_Negatives)\n",
    "Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def1e96",
   "metadata": {},
   "source": [
    "The ToposText annotation had a F1 score of 0.84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c716da0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.849965745604019"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate F1\n",
    "\n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
