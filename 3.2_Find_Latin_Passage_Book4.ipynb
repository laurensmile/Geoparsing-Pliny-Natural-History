{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bef2681",
   "metadata": {},
   "source": [
    "# Find the Latin passage Book 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a95cbbc",
   "metadata": {},
   "source": [
    "The notebook contains the code to get the attestation of a place name in the Latin text of the NH. This permits to access the Latin text of the NH starting from the English translation on ToposText. The edition of the NH is available on LacusCurtius (https://penelope.uchicago.edu/Thayer/E/Roman/Texts/Pliny_the_Elder/home.html).\n",
    "\n",
    "Starting from a place name attestation from the CSV file generated from ToposText (i.e., '4.24.2 Apollonia'), the corresponding Latin book_chapter is extracted from LacusCurtius. Each paragraph (p_tag) in the The Latin chapter (a_tag) is split into sentences (using NLTK) and the sentences into words. Then, the place name (i.e., 'Apollonia') is compared with all the words starting with capital letter in the text of the paragraph. A perfect match of the first three letters is expected between the place name and the target word, that is the Latin equivalent of the place name. The similarity (or Levenshtein distance) is calculated using the Python library FuzzyWuzzy. The similarity threshold ('score') is set to > 60, which was determined empirically. The code prints the target word detected (i.e., 'Apolloniam') and the score of the match.\n",
    "\n",
    "Notice that NLTK was used to split the paragraph into sentence, because there were some problems using the split function (i.e., division of M. Paulus).\n",
    "\n",
    "Notice also that Pliny's critical editions contain two different level of chapter subdivisions. To be able to switch from the English text (from ToposText) to the Latin text (LacusCurtius), it was necessary to manually create a table of correspondences (3.1.Table_(Chapter)Chapter_Paragraph). For each book, the table lists the first level of chapter subdivision (indicated between round bracket), the second level of chapter subdivision (not in round bracket) and the paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494740d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u0154817\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import webbrowser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from roman_arabic_numerals import conv\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d28974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "24\n",
      "Apollonia matched Apolloniam with score 95\n",
      "4.(24).78.0: M Varro ad hunc modum metitur ab ostio Ponti Apolloniam CLXXXVII·D p Callatim tantundem ad ostium Histri CXXV ad Borysthenem CCL Cherronesum Heracleotarum oppidum CCCLXXV p ad Panticapaeum quod aliqui Bosporum vocant extremum in Europae ora CCXII·D quae summa efficit XIII·XXXVII·D\n"
     ]
    }
   ],
   "source": [
    "Index_LacusCurtius=requests.get(\"http://penelope.uchicago.edu/Thayer/e/roman/texts/pliny_the_elder/home.html\") ## get the index from LacusCurtius\n",
    "Index_LacusCurtius_soup=BeautifulSoup(Index_LacusCurtius.content, 'html.parser') ## get the soup of the index\n",
    "base_url=\"https://penelope.uchicago.edu/Thayer/\" ## create a base url\n",
    "\n",
    "Book=input() ## select the book (book number i.e., '4')\n",
    "\n",
    "a_tag_Book = Index_LacusCurtius_soup.find('a', {'name': Book}) ## find the book in the index\n",
    "href=a_tag_Book.get(\"href\") ## get the href of the book in the index\n",
    "full_url=base_url+href.strip() ## get the full link of the book\n",
    "\n",
    "LacusCurtius_book=requests.get(full_url) ## open the book in LacusCurtius\n",
    "LacusCurtius_book_soup=BeautifulSoup(LacusCurtius_book.content) ## get the soup of the book\n",
    "\n",
    "Chapter=input() ## select the chapter\n",
    "Chapter=int(Chapter) ## convert to integer\n",
    "\n",
    "## get the table of correspondences\n",
    "Table_of_Correspondences=pd.read_csv(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/3.1.Table_(Chapter)_Chapter_Paragraph.csv\", delimiter=\";\")\n",
    "\n",
    "## filter the rows with the book and chapter numbers of the inputs\n",
    "Select_Rows=Table_of_Correspondences[(Table_of_Correspondences[\"Book\"]==int(Book)) & (Table_of_Correspondences[\"(Chapter)\"]==(Chapter))]\n",
    "\n",
    "List_of_Paragraphs=[] ## create a list\n",
    "Paragraphs=Select_Rows['Paragraph'].tolist() ## append all the paragraphs to the list\n",
    "Paragraphs=np.unique(Paragraphs) ## drop duplicates\n",
    "for Paragraph in Paragraphs:\n",
    "    Paragraph=int(Paragraph) ## convert to integer\n",
    "    List_of_Paragraphs.append(Paragraph)\n",
    "\n",
    "List_of_Chapters_no_brack=[] ## create a list\n",
    "Chapters_no_brack=Select_Rows['Chapter'].tolist() ## append all the chapters not in round brackets\n",
    "Chapters_no_brack=np.unique(Chapters_no_brack) ## drop duplicates\n",
    "for Chapter_no_brack in Chapters_no_brack:\n",
    "    Book_Chapter_no_brack=Book+\".\"+Chapter_no_brack ## create the string book.chapter_no_brackets\n",
    "    List_of_Chapters_no_brack.append(Book_Chapter_no_brack)\n",
    "\n",
    "    \n",
    "## the books of the NH can split into two groups according to the different chapter subivisions in the LacusCurtius version\n",
    "group_1=[\"2\", \"5\", \"6\", \"8\", \"9\", \"10\", \"11\"]\n",
    "group_2=[\"3\", \"4\", \"7\"]\n",
    "\n",
    "Place_Name = \"Apollonia\" ## English place name\n",
    "\n",
    "if Book in group_1:\n",
    "\n",
    "    Chapter_Roman=str(conv.arab_rom(Chapter).lower()) ## convert the chapter number into a Roman number\n",
    "    a_tag_Chapter = LacusCurtius_book_soup.find('a', {'id': Chapter_Roman}) ## find the chapter in the book\n",
    "    \n",
    "    if a_tag_Chapter: \n",
    "        \n",
    "        Succ_Chapter=Chapter+1 ## determine the number of the next chapter\n",
    "        Succ_Chapter_Roman=str(conv.arab_rom(Succ_Chapter).lower()) ## convert the number into a Roman number\n",
    "        end_tag=LacusCurtius_book_soup.find('a', {'id': Succ_Chapter_Roman}) ## find the next chapter\n",
    "        end_of_Book=LacusCurtius_book_soup.find('a', {'id': \"thanks\"}) ## find the end of the book\n",
    "\n",
    "        tag_p_count=-1 #count the number of p_tags in the chapter\n",
    "\n",
    "        for tag in a_tag_Chapter.find_all_next(): ## for all the next tags\n",
    "            if tag==end_tag: ## if the next tag is the next chapter\n",
    "                break ## break\n",
    "            if tag==end_of_Book: ## if the next tag is the end of the book\n",
    "                break ## break\n",
    "            if tag.name==\"p\": ## if the next tag is a p_tag (paragraph)\n",
    "                tag_p_count=tag_p_count+1 ## add +1 to the count of the p_tags\n",
    "                Text=tag.text ## get the text of the p_tag\n",
    "        \n",
    "                Sentences = sent_tokenize(Text) ## split the text into sentences using NLTK\n",
    "                for i,Sentence in enumerate(Sentences): ## for each sentence\n",
    "                    Sentence=Sentence.translate(str.maketrans('', '', string.punctuation)) #remove the punctuation from the sentence\n",
    "                    Words=Sentence.split() ## split the sentence into words\n",
    "                    for Word in Words: ## for each word\n",
    "                        if Word[0:3]==Place_Name[0:3]: #compare the beginning of the place name\n",
    "                            Score=fuzz.token_set_ratio(Place_Name,Word) ## get the fuzzy score of the distance between the place name and the word\n",
    "                            if Score>=60: ## if the matching score is higher than 60\n",
    "                                Target_Word=Word ## get the target word\n",
    "                                print(Place_Name, 'matched', Target_Word, 'with score', Score)\n",
    "                                \n",
    "                                ## determine the number of the paragraph and the number of the sentence\n",
    "                                if tag_p_count in range(len(List_of_Paragraphs)):\n",
    "                                    print(str(Book)+\".(\"+str(Chapter)+\").\"+str(List_of_Paragraphs[tag_p_count])+\".\"+str(i)+\":\"+Sentence.strip())\n",
    "                                else: print(str(Book)+\".(\"+str(Chapter)+\").CHECKTHEPARAGRAPH.\"+str(i)+\":\"+Sentence.strip())\n",
    "                                    \n",
    "        ## if no p_tag was detected\n",
    "        if tag_p_count==-1:\n",
    "    \n",
    "            ## check for a_tag containing the text of the chapter\n",
    "            for Paragraph in List_of_Paragraphs:\n",
    "                a_tag_Paragraph=LacusCurtius_book_soup.find(\"a\", {\"class\": \"chapter\", \"name\": Paragraph})\n",
    "        \n",
    "                all_a_tags=LacusCurtius_book_soup.find_all(\"a\")\n",
    "                index1=all_a_tags.index(a_tag_Chapter)\n",
    "                index2=all_a_tags.index(a_tag_Paragraph)\n",
    "        \n",
    "                #the paragraph follows the chapter number\n",
    "                if index1<index2:\n",
    "            \n",
    "                    Text=a_tag_Paragraph.find_next_sibling(text=True).strip()\n",
    "                    Sentences = sent_tokenize(Text)\n",
    "                    for i,Sentence in enumerate(Sentences):\n",
    "                        #remove the punctuation from the phrase\n",
    "                        Sentence=Sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "                        #split the phrase in words\n",
    "                        Words=Sentence.split()\n",
    "                        for Word in Words:\n",
    "                            #compare the beginning of the target word with each word\n",
    "                            if Word[0:3]==Place_Name[0:3]:\n",
    "                                Score=fuzz.token_set_ratio(Place_Name,Word)\n",
    "                                if Score>=60:\n",
    "                                    Target_Word=Word\n",
    "                                    print(Place_Name, 'matched', Target_Word, 'with score', Score)\n",
    "                                    print(str(Book)+\".(\"+str(Chapter)+\").\"+str(Paragraph)+\".\"+str(i)+\":\"+Sentence.strip())\n",
    "    \n",
    "                #the paragraph begins before the chapter number\n",
    "                elif index1>index2:\n",
    "            \n",
    "                    Text=a_tag_Chapter.find_next_sibling(text=True).strip()\n",
    "                \n",
    "                    Sentences = sent_tokenize(Text)\n",
    "                    for i,Sentence in enumerate(Sentences):\n",
    "                        #remove the punctuation from the phrase\n",
    "                        Sentence=Sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "                        #split the phrase in words\n",
    "                        Words=Sentence.split()\n",
    "                        for Word in Words:\n",
    "                            #compare the beginning of the target word with each word\n",
    "                            if Word[0:3]==Place_Name[0:3]:\n",
    "                                Score=fuzz.token_set_ratio(Place_Name,Word)\n",
    "                                if Score>=60:\n",
    "                                    Target_Word=Word\n",
    "                                    print(Place_Name, 'matched', Target_Word, 'with score', Score)\n",
    "                                    print(str(Book)+\".(\"+str(Chapter)+\").\"+str(Paragraph)+\".\"+str(i)+\":\"+Sentence.strip())\n",
    "\n",
    "       \n",
    "    else: print(\"No Chapter\")\n",
    "    \n",
    "elif Book in group_2:\n",
    "    \n",
    "    for Paragraph in List_of_Paragraphs: ## for all the paragraphs in the chapter\n",
    "        a_tag_Paragraph=LacusCurtius_book_soup.find(\"a\", {\"name\": Paragraph}) ## get the tags of the paragraphs\n",
    "        \n",
    "        ## extract the text from the next p_tag by td_parents\n",
    "        td_Paragraph_parent_tag=a_tag_Paragraph.parent\n",
    "        td_p_parent_tag=td_Paragraph_parent_tag.find_next_sibling(\"td\")\n",
    "        for p_tag in td_p_parent_tag.find_all(\"p\"):\n",
    "            Text=p_tag.text.strip()\n",
    "        \n",
    "            Sentences = sent_tokenize(Text) \n",
    "            for i,Sentence in enumerate(Sentences):\n",
    "                Sentence=Sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "                Words=Sentence.split()\n",
    "                for Word in Words:\n",
    "                    if Word[0:3]==Place_Name[0:3]:\n",
    "                        Score=fuzz.token_set_ratio(Place_Name,Word)\n",
    "                        if Score>=60:\n",
    "                            Target_Word=Word\n",
    "                            print(Place_Name, 'matched', Target_Word, 'with score', Score)\n",
    "                            print(str(Book)+\".(\"+str(Chapter)+\").\"+str(Paragraph)+\".\"+str(i)+\": \"+Sentence.strip())        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
