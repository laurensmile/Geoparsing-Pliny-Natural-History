{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "544734af",
   "metadata": {},
   "source": [
    "# Calculate the Extension of Book 4 (Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc4dcf",
   "metadata": {},
   "source": [
    "Before performing NER on Book 4, the extension of the text was calculated. Paragraphs were tokenized and stopwords were removed.\n",
    "\n",
    "NLTK counted 18,673 tokens and 10,880 tokens excluding stopwords in Book 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f21fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import BeautifulSoup\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0356f767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\u0154817\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import NLTK and stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')  # Download the necessary resources (only needs to be done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2033c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of Tokens: 18673\n",
      "Total of Tokens excluding stopwords: 10880\n"
     ]
    }
   ],
   "source": [
    "Lenght_Book4 = [] ## sum of tokens in Book 4\n",
    "Lenght_Book4_No_Stopwords = [] ## sum of tokens in Book 4 excluding stopwords\n",
    "\n",
    "## remove stop words\n",
    "Stop_words = set(stopwords.words('english'))\n",
    "\n",
    "## open the source page as soup\n",
    "soup = BeautifulSoup(open(\"/Users/u0154817/OneDrive - KU Leuven\\Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Sources/NH_Eng_ToposText/NH_Eng_1-11.html\", encoding='utf-8'), features=\"lxml\")\n",
    "\n",
    "## find all the paragraphs in book 4\n",
    "Book4 = soup.find_all(\"p\", id=lambda x: x and x.startswith(\"urn:cts:latinLit:phi0978.phi001:4.\"))\n",
    "\n",
    "for Paragraph in Book4: ## for each paragraph in book 4\n",
    "    Paragraph = Paragraph.get_text() ## get the text of the paragraph\n",
    "    \n",
    "    Tokens = nltk.word_tokenize(Paragraph) ## tokenize the text\n",
    "    Lenght_of_the_Paragraph = int(len(Tokens)) ## calculate the sum of tokens in the paragraph\n",
    "    Lenght_Book4.append(Lenght_of_the_Paragraph) ## append the sum of tokens at the lenght list\n",
    "    \n",
    "    Tokens_No_Stopwords=[Token for Token in Tokens if Token.lower() not in Stop_words] ## remove stopwords from the list of tokens\n",
    "    Lenght_of_Paragraph_No_Stopwords=int(len(Tokens_No_Stopwords)) ## calculate the sum of tokens in the paragraph excluding stopwords\n",
    "    Lenght_Book4_No_Stopwords.append(Lenght_of_Paragraph_No_Stopwords) ##append the sum of tokens at the list excluding stopwords\n",
    "\n",
    "Lenght_Book4 = sum(Lenght_Book4)\n",
    "print(\"Total of Tokens:\", Lenght_Book4)\n",
    "Lenght_Book4_No_Stopwords = sum(Lenght_Book4_No_Stopwords)\n",
    "print(\"Total of Tokens excluding stopwords:\", Lenght_Book4_No_Stopwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
