{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2032d74f",
   "metadata": {},
   "source": [
    "# Group tokens into Sentences in Book 4 (Latin)Â¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591defb2",
   "metadata": {},
   "source": [
    "The lemmatized version of the Natural History is obtained from the Lasciva Roma project by C. Thibault accessible at: https://github.com/lascivaroma/latin-lemmatized-texts/blob/main/lemmatized/xml/urn%3Acts%3AlatinLit%3Aphi0978.phi001.perseus-lat2.xml.\n",
    "\n",
    "In total, 519 sentences were counted in Book 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8916350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452ce0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Sources/NH_Lat_Lemm_Thibault/urn_cts_latinLit_phi0978.phi001.perseus-lat2.xml\", encoding='utf-8'), features=\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff5b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all the paragraphs in Book 4\n",
    "Book_4_lemmatized = soup.find_all(\"ab\", n=lambda x: x and x.startswith(\"urn:cts:latinLit:phi0978.phi001.perseus-lat2:4.\")) ## get all the paragraph starting with the ID phi0978.phi001:4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb660f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reference = []\n",
    "Token = []\n",
    "#Start_pos = []\n",
    "Lemma = []\n",
    "Msd = []\n",
    "Pos = []\n",
    "\n",
    "start_pos = 0\n",
    "\n",
    "urn = 'urn:cts:latinLit:phi0978.phi001.perseus-lat2:'\n",
    "\n",
    "for ab_tag in Book_4_lemmatized:\n",
    "    w_tags = ab_tag.find_all(\"w\") ## find all w tags\n",
    "    \n",
    "    for w_tag in w_tags:\n",
    "        \n",
    "        book_chapter = w_tag.get(\"n\") ## get the book chapter\n",
    "        reference = urn+book_chapter\n",
    "        Reference.append(reference)\n",
    "        \n",
    "        token = w_tag.get_text() ## get the token\n",
    "        Token.append(token)\n",
    "        \n",
    "        #Start_pos.append(start_pos) ## compute the start position\n",
    "        #length_word = len(token)+2\n",
    "        #start_pos = start_pos + length_word\n",
    "        \n",
    "        lemma = w_tag.get(\"lemma\") ## get the lemma\n",
    "        Lemma.append(lemma)\n",
    "        \n",
    "        msd = w_tag.get('msd') ## get the msd\n",
    "        Msd.append(msd)\n",
    "        pos = w_tag.get('pos') ## get the pos\n",
    "        Pos.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09104cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## concatenate the lists in a pandas dataframe\n",
    "\n",
    "data = {'Reference': Reference, 'Token': Token, 'Lemma': Lemma, 'msd': Msd, \"pos\": Pos}\n",
    "Book_4 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff8527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sentence = 0\n",
    "sentences_list = []\n",
    "ids_list = []\n",
    "punctuations = ['.', '!']\n",
    "\n",
    "for i, token in enumerate(Book_4['Token']):\n",
    "    sentences_list.append(count_sentence)\n",
    "    token_ID = str(Book_4['Reference'][i])+'.'+str(count_sentence)+'.'+str(Book_4['Start_pos'][i])\n",
    "    ids_list.append(token_ID)\n",
    "    if token in punctuations:\n",
    "        count_sentence = count_sentence + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "905378fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Book_4['Sentence'] = sentences_list\n",
    "Book_4['Token_ID'] = ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c98da4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tertius Europae sinus Acrocerauniis incipit montibus , finitur Hellesponto , amplectitur praeter minores sinus | XIX | .\n"
     ]
    }
   ],
   "source": [
    "## test: print the sentence 0\n",
    "\n",
    "## filter all the rows containing sentence = 0\n",
    "filtered_rows = Book_4[Book_4['Sentence'] == 0]\n",
    "\n",
    "## concatenate the tokens in filtered_rows\n",
    "concatenated_string = ' '.join(filtered_rows['Token'].astype(str))\n",
    "\n",
    "print(concatenated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab7e0a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "518"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cd20c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write a file containing the sentences\n",
    "\n",
    "with open(\"4.4.Sentence_Lat\", \"w\", encoding='utf-8') as file:\n",
    "    for i in range(0, max(sentences_list)+1):\n",
    "        filter_rows = Book_4[Book_4['Sentence'] == i]\n",
    "        concatenate_sentence = ' '.join(filter_rows['Token'].astype(str))\n",
    "        file.write(f\"{concatenate_sentence}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
