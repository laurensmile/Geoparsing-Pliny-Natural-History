{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5348c9b",
   "metadata": {},
   "source": [
    "# Evaluation of ToposText Annotation in Book 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da7682",
   "metadata": {},
   "source": [
    "To evalute the quality of the ToposText annotation both for the entity annotation and the labelling, Precision, Recall and F1 Score were calculated against a manually curated Gold Standard for Book 4. Firstly, the ToposText annotations were converted to the IOB format using the reference and start position. \n",
    "\n",
    "ToposText contains a good-quality annotation (F1 0.822) with high Precision (0.989). The Recall, instead, is lower (0.703), indicating that some entities were not annotated. More specifically, we counted 1,295 true positives, 635 false negatives, and 16 false positives.\n",
    "\n",
    "Some conclusions from this step:\n",
    "\n",
    "- In some cases, the place entity is annotated in ToposText, but the label is incorrect (i.e., 'Asia' is linked to a 'people' ToposText ID).\n",
    "- In some cases, the ToposText labelling is inconsistent (i.e., the entity is labelled as 'demonym' or 'ethnic' in Class and 'place' in the ToposText ID).\n",
    "- Using the 'Class' labelling, the ToposText annotation has a higher Precision rather than using the ToposText ID labelling, but a lower Recall.\n",
    "\n",
    "To sum up, ToposText is a solid foundation for the annotation process, but it is necessary to expand the existing annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6928be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open the Gold Standard of Book 4 (18,664 rows)\n",
    "GoldStandard_Book4 = pd.read_excel(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/1.3.GoldStandard_Book4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GoldStandard_Book4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open the file containing the ToposText annotations in Book 4 (1,888 rows)\n",
    "ToposText_Book4 = pd.read_csv(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/1.1.ToposText_Annotations_Book_4.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ToposText_Book4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f4d3b",
   "metadata": {},
   "source": [
    "# Convert ToposText annotation to IOB format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a1f11",
   "metadata": {},
   "source": [
    "The ToposText annotations of Class and ToposText ID are appended to the Gold Standard dataframe using the reference and start position. In case of multi-word entities annotated together in ToposText (i.e., 'Corinthian Gulf'), each word is annotated separately in the dataset.\n",
    "\n",
    "Notice that despite some ToposText annotations do not contain a Class, all the annotations are linked to a ToposText ID, as it was observed in the notebook '1.3.Explore_ToposText_Annotations_Book4'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dfab0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create two new columns in the Gold Standard dataframe\n",
    "GoldStandard_Book4['ToposText'] = 'O' ## column for the Class\n",
    "GoldStandard_Book4['ToposText_ID'] = 'O' ## column for the ToposText ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43641ff4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i1, topostext_annotation in enumerate(ToposText_Book4['Tagged Entity']): ## for each ToposText annotation \n",
    "        \n",
    "    reference = ToposText_Book4['Reference'][i1] ## get the reference (book, chapter, paragraph)\n",
    "    start_position = ToposText_Book4['Start position'][i1] ## get the start position\n",
    "    \n",
    "    for i2, manual_annotation in enumerate(GoldStandard_Book4['Token']): ## for each token in the Gold Standard\n",
    "        \n",
    "        if GoldStandard_Book4['Start_pos'][i2] == start_position: ## if the start position is the same\n",
    "            if GoldStandard_Book4['Reference'][i2] == reference: ## if the reference is the same\n",
    "                                \n",
    "                GoldStandard_Book4['ToposText'][i2] = ToposText_Book4['Class'][i1] ## update the ToposText column\n",
    "                GoldStandard_Book4['ToposText_ID'][i2] = ToposText_Book4['ToposText ID'][i1] ## update the ToposText ID column\n",
    "                \n",
    "                topostext_annotation = topostext_annotation.split() ## split the text annotated in ToposText\n",
    "                if len(topostext_annotation) > 1: ## if the annotation contains more than one word\n",
    "                                        \n",
    "                    for i3, word in enumerate(topostext_annotation): ## for each word\n",
    "                        \n",
    "                        if i3 > 0: ## except the first one\n",
    "                                                        \n",
    "                            GoldStandard_Book4['ToposText'][i2+i3] = ToposText_Book4['Class'][i1] ## update the corresponding ToposText column\n",
    "                            GoldStandard_Book4['ToposText_ID'][i2+i3] = ToposText_Book4['ToposText ID'][i1] ## update the corresponding ToposText ID column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0cb5d5",
   "metadata": {},
   "source": [
    "# Calculate Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f9d1a",
   "metadata": {},
   "source": [
    "To calculate the Precision, Recall and F1 Score, the lists of True Positives (TP), False Negatives (FN) and False Positives (FP) are generated from the dataset containing the Gold Standard and the ToposText annotations. A TP is a ToposText annotation that (a) is present in the Gold Standard and (b) is labelled as 'place' in ToposText. A FN is (a) an annotation that is not present in ToposText, but is present in the Gold Standard or (b) it is not correctly annotated as 'place'. A FP is an annotation that is annotated in ToposText as 'place', but not in the Gold Standard.\n",
    "\n",
    "The code takes into account that different guidelines and entity boundaries were adopted in the Gold Standard and in ToposText, with the result that the same entity (i.e., 'Mount Pindus') could be annotated in different ways (i.e., including or not the word 'Mount'). An annotation was counted as TP even if only a part of the entity was present in ToposText (i.e., 'Pindus' for 'Mount Pindus'). \n",
    "\n",
    "The evaluation of the labelling was performed inspecting the 'Class' label and the ToposText ID (i.e., https://topostext.org/place/395208SDod). It was observed that in some cases the labelling is inconsistent (i.e., the entity is classified as 'person' in 'Class' but it is linked to a 'place' ToposText ID). Firstly, the annotation was counted as a TP if the Class _or_ the ToposText ID correctly contained the 'place' label. Then, we compared the Precision of the annotation considering separately (1) the ToposText ID labelling and (2) the 'Class' labelling . We observed that using the 'Class' labelling instead of the ToposText ID the annotation has a lower Recall, but higher Precision. In other words, the number of FN increases (from 599 to 635), but the number of FP drops to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d320cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_Positives_label = [] ## entities correctly annotated in ToposText and labelled as 'places'\n",
    "False_Negatives = [] ## entities not annotated in ToposText or not labelled as 'places'\n",
    "\n",
    "for index, manual_annotation in enumerate(GoldStandard_Book4['Manual_Annotation']):\n",
    "        \n",
    "    if manual_annotation == 'B-LOC': ## for each B-entity in the Gold Standard\n",
    "        reference_startpos = (GoldStandard_Book4['Reference'][index], GoldStandard_Book4['Start_pos'][index])\n",
    "        \n",
    "        if len(GoldStandard_Book4['ToposText_ID'][index]) > 1: ## if it is annotated in ToposText\n",
    "            \n",
    "            if 'place' in str(GoldStandard_Book4['ToposText'][index]): ## if the ToposText annotation contains 'place' \n",
    "                True_Positives_label.append(reference_startpos) ## it is a true positive ToposText annotation\n",
    "            else: False_Negatives.append(reference_startpos) ## it is a false negative ToposText annotation\n",
    "        \n",
    "        if len(GoldStandard_Book4['ToposText_ID'][index]) == 1: ## if it is not annotated in ToposText\n",
    "            \n",
    "            if GoldStandard_Book4['Manual_Annotation'][index+1] != 'I-LOC': ## if it is not followed by I-LOC\n",
    "                False_Negatives.append(reference_startpos) ## it is a false negative ToposText annotation\n",
    "                \n",
    "            else: ## if it is in a multi-word entity\n",
    "                \n",
    "                flag = False\n",
    "                \n",
    "                for n in range(1,100): ## for any natural number\n",
    "                    \n",
    "                    if GoldStandard_Book4['Manual_Annotation'][index+n] == 'I-LOC': ## if it is followed by a I-LOC entity \n",
    "                        if len(GoldStandard_Book4['ToposText_ID'][index+n]) > 1: ## the I-LOC contains a ToposText ID\n",
    "                            if 'place' in str(GoldStandard_Book4['ToposText'][index+n]): ## if the ToposText annotation contains 'place'\n",
    "                                True_Positives_label.append((GoldStandard_Book4['Reference'][index+n], GoldStandard_Book4['Start_pos'][index+n])) ## it is a true positive ToposText annotation\n",
    "                                flag = True\n",
    "                                break\n",
    "                    else: break\n",
    "                        \n",
    "                if flag == False:\n",
    "                    False_Negatives.append(reference_startpos) ## it is a false negative ToposText annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f648fec",
   "metadata": {},
   "source": [
    "The ToposText annotation contains 1,295 true positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d17aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(True_Positives_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866502e",
   "metadata": {},
   "source": [
    "The ToposText annotation contains 635 false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(False_Negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d29da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## slice the dataframe showing only the O entities\n",
    "GoldStandard_Book4_O = GoldStandard_Book4[GoldStandard_Book4['Manual_Annotation'] == 'O']\n",
    "GoldStandard_Book4_O.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4241df",
   "metadata": {},
   "outputs": [],
   "source": [
    "False_Positives = [] ## entities annotated as 'places' in ToposText but not in the Gold Standard\n",
    "\n",
    "for index, manual_annotation in enumerate(GoldStandard_Book4_O['Manual_Annotation']):\n",
    "    \n",
    "    reference_startpos = (GoldStandard_Book4_O['Reference'][index], GoldStandard_Book4_O['Start_pos'][index])\n",
    "\n",
    "    if 'place' in str(GoldStandard_Book4_O['ToposText'][index]): ## if the ToposText annotation contains 'place'\n",
    "        False_Positives.append(reference_startpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f6e32a",
   "metadata": {},
   "source": [
    "The ToposText annotation contains 16 false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6dca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(False_Positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0f9de",
   "metadata": {},
   "source": [
    "The ToposText annotation has a Precision of 0.989."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate precision\n",
    "\n",
    "Precision = len(True_Positives) / (len(True_Positives) + len(False_Positives))\n",
    "Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ba693",
   "metadata": {},
   "source": [
    "The ToposText annotation has a Recall of 0.703."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate recall\n",
    "\n",
    "Recall = len(True_Positives) / (len(True_Positives) + len(False_Negatives))\n",
    "Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def1e96",
   "metadata": {},
   "source": [
    "The ToposText annotation has a F1 score of 0.822."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate F1\n",
    "\n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
