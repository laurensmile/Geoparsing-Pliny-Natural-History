{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4ca1a1",
   "metadata": {},
   "source": [
    "# Evaluate the Enriched ToposText Annotation in Book 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f201bf6d",
   "metadata": {},
   "source": [
    "To evalute the quality of the enriched ToposText annotation, we calculated the Precision, Recall and F1 Score against a manually curated Gold Standard for Book 4.\n",
    "\n",
    "In comparison to the ToposText annotation (Precision: 0.985, Recall: 0.747) and the Flair NER (Precision: 0.937, Recall: 0.955), the combination of the outputs shows good results both in terms of Precision (0.982) and Recall (0.989) with an improvement of the ToposText annotation (+24.2% Recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f16446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17bf2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open the Gold Standard for Book 4 (2,491 entries)\n",
    "GoldStandard_Book4 = pd.read_excel(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/1.3.GoldStandard_Book4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945bde81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2491"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GoldStandard_Book4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894d9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open the file containing the enriched ToposText annotation in Book 4 (2,508 rows)\n",
    "Enriched_ToposText_Book4 = pd.read_csv(\"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/1.8.Enriched_ToposText_Book4.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1025aaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2508"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Enriched_ToposText_Book4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53082adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a set of tuples for the Gold Standard\n",
    "GoldStandard_tuples = set(zip(GoldStandard_Book4['Reference'], GoldStandard_Book4['First position']))\n",
    "\n",
    "## create a set of tuples for the Flair NER\n",
    "EnrichedToposText_Book4_tuples = set(zip(Enriched_ToposText_Book4['Reference'], Enriched_ToposText_Book4['Start position']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f446c8",
   "metadata": {},
   "source": [
    "The enriched annotation contains 2,381 true positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b911432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2465"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate true positives\n",
    "\n",
    "True_Positives = len(EnrichedToposText_Book4_tuples.intersection(GoldStandard_tuples))\n",
    "True_Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506b84f",
   "metadata": {},
   "source": [
    "The enriched annotation contains 43 false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5006e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate false positives\n",
    "\n",
    "False_Positives = len(EnrichedToposText_Book4_tuples) - True_Positives\n",
    "False_Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a82320",
   "metadata": {},
   "source": [
    "The enriched annotation has a precision of 0.982."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5d4a63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9828548644338118"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate the precision\n",
    "\n",
    "Precision = True_Positives / (True_Positives + False_Positives)\n",
    "Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fee8a3",
   "metadata": {},
   "source": [
    "The enriched annotation contains 26 false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855b0bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate false negatives\n",
    "\n",
    "False_Negatives = len(GoldStandard_tuples - EnrichedToposText_Book4_tuples)\n",
    "False_Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c7f1af",
   "metadata": {},
   "source": [
    "The enriched annotation has a recall of 0.989."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56351996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9895624247290244"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate recall\n",
    "\n",
    "Recall = True_Positives / (True_Positives + False_Negatives)\n",
    "Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8369f",
   "metadata": {},
   "source": [
    "The enriched annotation has a F1 score of 0.986."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d51dceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861972394478895"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate F1\n",
    "\n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
